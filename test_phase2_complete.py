"""
AIFX Phase 2 Comprehensive Test Suite | AIFXç¬¬äºŒéšæ®µç¶œåˆæ¸¬è©¦å¥—ä»¶

This script performs comprehensive testing of all Phase 2 AI model components.
æ­¤è…³æœ¬å°æ‰€æœ‰ç¬¬äºŒéšæ®µAIæ¨¡å‹çµ„ä»¶é€²è¡Œç¶œåˆæ¸¬è©¦ã€‚

Usage | ä½¿ç”¨æ–¹æ³•:
    python test_phase2_complete.py

Features | åŠŸèƒ½:
- Base model framework validation | åŸºç¤æ¨¡å‹æ¡†æ¶é©—è­‰
- XGBoost model implementation test | XGBoostæ¨¡å‹å¯¦ç¾æ¸¬è©¦
- Random Forest model implementation test | éš¨æ©Ÿæ£®æ—æ¨¡å‹å¯¦ç¾æ¸¬è©¦
- LSTM model implementation test | LSTMæ¨¡å‹å¯¦ç¾æ¸¬è©¦
- Training pipeline validation | è¨“ç·´ç®¡é“é©—è­‰
- Performance metrics testing | æ€§èƒ½æŒ‡æ¨™æ¸¬è©¦
- Model management system test | æ¨¡å‹ç®¡ç†ç³»çµ±æ¸¬è©¦
"""

import sys
import os
import traceback
import warnings
from datetime import datetime
from pathlib import Path
import numpy as np
import pandas as pd

warnings.filterwarnings('ignore')

# Add src path for imports | ç‚ºå°å…¥æ·»åŠ srcè·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src', 'main', 'python'))

class Colors:
    """Console colors for output formatting | è¼¸å‡ºæ ¼å¼çš„æ§åˆ¶å°é¡è‰²"""
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'

class Phase2Tester:
    """
    Comprehensive Phase 2 AI model testing class | ç¶œåˆç¬¬äºŒéšæ®µAIæ¨¡å‹æ¸¬è©¦é¡
    """
    
    def __init__(self):
        """Initialize the tester | åˆå§‹åŒ–æ¸¬è©¦å™¨"""
        self.test_results = []
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
        self.warnings = []
        
        # Test start time | æ¸¬è©¦é–‹å§‹æ™‚é–“
        self.start_time = datetime.now()
        
        print(f"{Colors.BOLD}{Colors.BLUE}ğŸ¤– AIFX Phase 2 AI Model Testing Suite{Colors.END}")
        print(f"{Colors.CYAN}Testing AI model implementations... | æ­£åœ¨æ¸¬è©¦AIæ¨¡å‹å¯¦ç¾...{Colors.END}\n")
    
    def log_test(self, test_name: str, passed: bool, message: str = "", warning: str = ""):
        """Log test result | è¨˜éŒ„æ¸¬è©¦çµæœ"""
        self.total_tests += 1
        
        if passed:
            self.passed_tests += 1
            status = f"{Colors.GREEN}âœ… PASS{Colors.END}"
        else:
            self.failed_tests += 1
            status = f"{Colors.RED}âŒ FAIL{Colors.END}"
        
        self.test_results.append({
            'test': test_name,
            'passed': passed,
            'message': message,
            'warning': warning
        })
        
        print(f"{status} {test_name}")
        if message:
            print(f"    {Colors.WHITE}{message}{Colors.END}")
        if warning:
            print(f"    {Colors.YELLOW}âš ï¸ {warning}{Colors.END}")
            self.warnings.append(warning)
    
    def create_sample_data(self, n_samples=1000):
        """Create sample forex data for testing | å‰µå»ºç”¨æ–¼æ¸¬è©¦çš„æ¨£æœ¬å¤–åŒ¯æ•¸æ“š"""
        dates = pd.date_range('2024-01-01', periods=n_samples, freq='H')
        
        # Generate realistic forex price movements | ç”ŸæˆçœŸå¯¦çš„å¤–åŒ¯åƒ¹æ ¼è®Šå‹•
        base_price = 1.1000
        returns = np.random.normal(0, 0.001, n_samples)
        prices = [base_price]
        
        for ret in returns[1:]:
            prices.append(prices[-1] * (1 + ret))
        
        # Create OHLCV data | å‰µå»ºOHLCVæ•¸æ“š
        data = pd.DataFrame({
            'Open': [p + np.random.normal(0, 0.0001) for p in prices],
            'High': [p + abs(np.random.normal(0, 0.0003)) for p in prices],
            'Low': [p - abs(np.random.normal(0, 0.0003)) for p in prices],
            'Close': prices,
            'Volume': np.random.uniform(1000, 5000, n_samples)
        }, index=dates)
        
        # Fix OHLC relationships | ä¿®å¾©OHLCé—œä¿‚
        for i in range(len(data)):
            max_oc = max(data.iloc[i]['Open'], data.iloc[i]['Close'])
            min_oc = min(data.iloc[i]['Open'], data.iloc[i]['Close'])
            data.iloc[i]['High'] = max(data.iloc[i]['High'], max_oc)
            data.iloc[i]['Low'] = min(data.iloc[i]['Low'], min_oc)
        
        return data
    
    def test_ai_dependencies(self):
        """Test AI-specific dependencies | æ¸¬è©¦AIç‰¹å®šä¾è³´é …"""
        print(f"{Colors.BOLD}1. AI Dependencies Tests | AIä¾è³´é …æ¸¬è©¦{Colors.END}")
        
        # Test deep learning dependencies | æ¸¬è©¦æ·±åº¦å­¸ç¿’ä¾è³´é …
        ai_deps = [
            ('tensorflow', 'Deep learning framework'),
            ('keras', 'High-level neural networks API'), 
            ('xgboost', 'Gradient boosting framework'),
            ('joblib', 'Model serialization'),
            ('sklearn', 'Machine learning library')
        ]
        
        for dep, description in ai_deps:
            try:
                if dep == 'sklearn':
                    import sklearn
                else:
                    __import__(dep)
                self.log_test(f"AI Dependency: {dep}", True, description)
            except ImportError:
                self.log_test(f"AI Dependency: {dep}", False, f"Missing: {description}")
    
    def test_base_model_framework(self):
        """Test base model framework | æ¸¬è©¦åŸºç¤æ¨¡å‹æ¡†æ¶"""
        print(f"\n{Colors.BOLD}2. Base Model Framework Tests | åŸºç¤æ¨¡å‹æ¡†æ¶æ¸¬è©¦{Colors.END}")
        
        try:
            # Test imports | æ¸¬è©¦å°å…¥
            from models.base_model import BaseModel, ModelRegistry
            self.log_test("Base Model Import", True)
            
            # Test ModelRegistry | æ¸¬è©¦æ¨¡å‹è¨»å†Šè¡¨
            registry = ModelRegistry()
            self.log_test("Model Registry Creation", True)
            
            # Test registry operations | æ¸¬è©¦è¨»å†Šè¡¨æ“ä½œ
            registry.register_model("test_model", "TestModel", "1.0")
            models = registry.list_models()
            if "test_model" in models:
                self.log_test("Model Registration", True, "Model registered successfully")
            else:
                self.log_test("Model Registration", False, "Failed to register model")
            
            # Test abstract methods exist | æ¸¬è©¦æŠ½è±¡æ–¹æ³•å­˜åœ¨
            abstract_methods = ['train', 'predict', 'evaluate', 'save_model', 'load_model']
            for method in abstract_methods:
                if hasattr(BaseModel, method):
                    self.log_test(f"Abstract Method: {method}", True)
                else:
                    self.log_test(f"Abstract Method: {method}", False, "Method not found")
            
        except Exception as e:
            self.log_test("Base Model Framework", False, f"Error: {str(e)}")
    
    def test_xgboost_model(self):
        """Test XGBoost model implementation | æ¸¬è©¦XGBoostæ¨¡å‹å¯¦ç¾"""
        print(f"\n{Colors.BOLD}3. XGBoost Model Tests | XGBoostæ¨¡å‹æ¸¬è©¦{Colors.END}")
        
        try:
            # Test import | æ¸¬è©¦å°å…¥
            from models.xgboost_model import XGBoostModel
            self.log_test("XGBoost Model Import", True)
            
            # Test model creation | æ¸¬è©¦æ¨¡å‹å‰µå»º
            model = XGBoostModel()
            self.log_test("XGBoost Model Creation", True)
            
            # Create sample data | å‰µå»ºæ¨£æœ¬æ•¸æ“š
            sample_data = self.create_sample_data(500)
            
            # Add basic features | æ·»åŠ åŸºæœ¬ç‰¹å¾µ
            sample_data['returns'] = sample_data['Close'].pct_change()
            sample_data['sma_10'] = sample_data['Close'].rolling(10).mean()
            sample_data['rsi'] = self._calculate_rsi(sample_data['Close'])
            sample_data = sample_data.dropna()
            
            if len(sample_data) < 100:
                self.log_test("XGBoost Data Preparation", False, "Insufficient data after preprocessing")
                return
                
            # Prepare features and target | æº–å‚™ç‰¹å¾µå’Œç›®æ¨™
            feature_cols = ['returns', 'sma_10', 'rsi']
            X = sample_data[feature_cols].fillna(method='ffill').fillna(method='bfill')
            y = (sample_data['Close'].shift(-1) > sample_data['Close']).astype(int)
            y = y[:-1]  # Remove last NaN
            X = X[:-1]  # Match lengths
            
            if len(X) != len(y):
                self.log_test("XGBoost Data Alignment", False, f"X shape: {X.shape}, y length: {len(y)}")
                return
            
            # Split data | åˆ†å‰²æ•¸æ“š
            split_idx = int(0.8 * len(X))
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            self.log_test("XGBoost Data Preparation", True, 
                         f"Train: {len(X_train)}, Test: {len(X_test)}")
            
            # Test training | æ¸¬è©¦è¨“ç·´
            training_history = model.train(X_train, y_train)
            if training_history and model.is_trained:
                self.log_test("XGBoost Training", True, "Model trained successfully")
            else:
                self.log_test("XGBoost Training", False, "Training failed")
                return
            
            # Test prediction | æ¸¬è©¦é æ¸¬
            predictions = model.predict(X_test)
            if len(predictions) == len(X_test):
                self.log_test("XGBoost Prediction", True, f"Predicted {len(predictions)} samples")
            else:
                self.log_test("XGBoost Prediction", False, "Prediction size mismatch")
            
            # Test probability prediction | æ¸¬è©¦æ¦‚ç‡é æ¸¬
            probabilities = model.predict_proba(X_test)
            if probabilities is not None and len(probabilities) == len(X_test):
                self.log_test("XGBoost Probabilities", True, "Probability prediction working")
            else:
                self.log_test("XGBoost Probabilities", False, "Probability prediction failed")
            
            # Test feature importance | æ¸¬è©¦ç‰¹å¾µé‡è¦æ€§
            importance = model.get_feature_importance()
            if importance is not None and len(importance) == len(feature_cols):
                self.log_test("XGBoost Feature Importance", True, "Feature importance extracted")
            else:
                self.log_test("XGBoost Feature Importance", False, "Feature importance failed")
            
        except Exception as e:
            self.log_test("XGBoost Model", False, f"Error: {str(e)}")
            traceback.print_exc()
    
    def test_random_forest_model(self):
        """Test Random Forest model implementation | æ¸¬è©¦éš¨æ©Ÿæ£®æ—æ¨¡å‹å¯¦ç¾"""
        print(f"\n{Colors.BOLD}4. Random Forest Model Tests | éš¨æ©Ÿæ£®æ—æ¨¡å‹æ¸¬è©¦{Colors.END}")
        
        try:
            # Test import | æ¸¬è©¦å°å…¥
            from models.random_forest_model import RandomForestModel
            self.log_test("Random Forest Model Import", True)
            
            # Test model creation | æ¸¬è©¦æ¨¡å‹å‰µå»º
            model = RandomForestModel()
            self.log_test("Random Forest Model Creation", True)
            
            # Use same sample data as XGBoost | ä½¿ç”¨èˆ‡XGBoostç›¸åŒçš„æ¨£æœ¬æ•¸æ“š
            sample_data = self.create_sample_data(400)
            
            # Add features | æ·»åŠ ç‰¹å¾µ
            sample_data['returns'] = sample_data['Close'].pct_change()
            sample_data['sma_5'] = sample_data['Close'].rolling(5).mean()
            sample_data['volume_ma'] = sample_data['Volume'].rolling(10).mean()
            sample_data = sample_data.dropna()
            
            if len(sample_data) < 100:
                self.log_test("Random Forest Data Preparation", False, "Insufficient data")
                return
            
            # Prepare features and target | æº–å‚™ç‰¹å¾µå’Œç›®æ¨™
            feature_cols = ['returns', 'sma_5', 'volume_ma']
            X = sample_data[feature_cols].fillna(method='ffill').fillna(method='bfill')
            y = (sample_data['Close'].shift(-1) > sample_data['Close']).astype(int)
            y = y[:-1]
            X = X[:-1]
            
            # Split data | åˆ†å‰²æ•¸æ“š
            split_idx = int(0.8 * len(X))
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            self.log_test("Random Forest Data Preparation", True,
                         f"Train: {len(X_train)}, Test: {len(X_test)}")
            
            # Test training | æ¸¬è©¦è¨“ç·´
            training_history = model.train(X_train, y_train)
            if training_history and model.is_trained:
                self.log_test("Random Forest Training", True, "Model trained successfully")
            else:
                self.log_test("Random Forest Training", False, "Training failed")
                return
            
            # Test prediction | æ¸¬è©¦é æ¸¬
            predictions = model.predict(X_test)
            if len(predictions) == len(X_test):
                self.log_test("Random Forest Prediction", True, f"Predicted {len(predictions)} samples")
            else:
                self.log_test("Random Forest Prediction", False, "Prediction size mismatch")
            
            # Test ensemble statistics | æ¸¬è©¦é›†æˆçµ±è¨ˆ
            ensemble_stats = model.get_ensemble_statistics()
            if ensemble_stats and 'n_estimators' in ensemble_stats:
                self.log_test("Random Forest Ensemble Stats", True, 
                             f"Trees: {ensemble_stats['n_estimators']}")
            else:
                self.log_test("Random Forest Ensemble Stats", False, "Ensemble stats failed")
            
        except Exception as e:
            self.log_test("Random Forest Model", False, f"Error: {str(e)}")
            traceback.print_exc()
    
    def test_lstm_model(self):
        """Test LSTM model implementation | æ¸¬è©¦LSTMæ¨¡å‹å¯¦ç¾"""
        print(f"\n{Colors.BOLD}5. LSTM Model Tests | LSTMæ¨¡å‹æ¸¬è©¦{Colors.END}")
        
        try:
            # Test import | æ¸¬è©¦å°å…¥
            from models.lstm_model import LSTMModel
            self.log_test("LSTM Model Import", True)
            
            # Test model creation | æ¸¬è©¦æ¨¡å‹å‰µå»º  
            model = LSTMModel()
            self.log_test("LSTM Model Creation", True)
            
            # Create time series data | å‰µå»ºæ™‚é–“åºåˆ—æ•¸æ“š
            sample_data = self.create_sample_data(200)
            
            # Prepare sequence data | æº–å‚™åºåˆ—æ•¸æ“š
            prices = sample_data['Close'].values
            sequence_length = 10
            
            # Create sequences | å‰µå»ºåºåˆ—
            X, y = [], []
            for i in range(sequence_length, len(prices)):
                X.append(prices[i-sequence_length:i])
                y.append(1 if prices[i] > prices[i-1] else 0)
            
            X = np.array(X).reshape((len(X), sequence_length, 1))
            y = np.array(y)
            
            if len(X) < 50:
                self.log_test("LSTM Data Preparation", False, "Insufficient sequence data")
                return
            
            # Split data | åˆ†å‰²æ•¸æ“š
            split_idx = int(0.8 * len(X))
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            self.log_test("LSTM Data Preparation", True,
                         f"Sequences: {X.shape}, Train: {len(X_train)}")
            
            # Test training (with short epochs for testing) | æ¸¬è©¦è¨“ç·´ï¼ˆç”¨çŸ­å‘¨æœŸé€²è¡Œæ¸¬è©¦ï¼‰
            training_history = model.train(
                X_train, y_train, 
                X_val=X_test, y_val=y_test,
                epochs=2,  # Short for testing | æ¸¬è©¦ç”¨çŸ­å‘¨æœŸ
                batch_size=16,
                verbose=0
            )
            
            if training_history and model.is_trained:
                self.log_test("LSTM Training", True, "Model trained successfully")
            else:
                self.log_test("LSTM Training", False, "Training failed")
                return
            
            # Test prediction | æ¸¬è©¦é æ¸¬
            predictions = model.predict(X_test)
            if len(predictions) == len(X_test):
                self.log_test("LSTM Prediction", True, f"Predicted {len(predictions)} samples")
            else:
                self.log_test("LSTM Prediction", False, "Prediction size mismatch")
                
        except Exception as e:
            self.log_test("LSTM Model", False, f"Error: {str(e)}")
            # Note: TensorFlow might not be available in all environments
            # æ³¨æ„ï¼šTensorFlowå¯èƒ½åœ¨æ‰€æœ‰ç’°å¢ƒä¸­éƒ½ä¸å¯ç”¨
            if "tensorflow" in str(e).lower() or "keras" in str(e).lower():
                self.warnings.append("TensorFlow/Keras required for LSTM - install with: pip install tensorflow")
    
    def test_training_pipeline(self):
        """Test training pipeline | æ¸¬è©¦è¨“ç·´ç®¡é“"""
        print(f"\n{Colors.BOLD}6. Training Pipeline Tests | è¨“ç·´ç®¡é“æ¸¬è©¦{Colors.END}")
        
        try:
            # Test import | æ¸¬è©¦å°å…¥
            from training.model_pipeline import ModelTrainingPipeline
            self.log_test("Training Pipeline Import", True)
            
            # Test pipeline creation | æ¸¬è©¦ç®¡é“å‰µå»º
            pipeline = ModelTrainingPipeline()
            self.log_test("Training Pipeline Creation", True)
            
            # Create sample data for pipeline | ç‚ºç®¡é“å‰µå»ºæ¨£æœ¬æ•¸æ“š
            sample_data = self.create_sample_data(300)
            
            # Add technical indicators using Phase 1 infrastructure | ä½¿ç”¨ç¬¬ä¸€éšæ®µåŸºç¤è¨­æ–½æ·»åŠ æŠ€è¡“æŒ‡æ¨™
            from utils.technical_indicators import TechnicalIndicators
            from utils.data_preprocessor import DataPreprocessor
            
            ti = TechnicalIndicators()
            preprocessor = DataPreprocessor()
            
            # Add indicators | æ·»åŠ æŒ‡æ¨™
            data_with_indicators = ti.add_all_indicators(sample_data)
            processed_data = preprocessor.preprocess_data(data_with_indicators)
            
            if len(processed_data) < 100:
                self.log_test("Pipeline Data Preparation", False, "Insufficient processed data")
                return
            
            # Get features and target | ç²å–ç‰¹å¾µå’Œç›®æ¨™
            X, y = preprocessor.get_features_and_target(processed_data)
            
            if len(X) == 0 or len(y) == 0:
                self.log_test("Pipeline Data Preparation", False, "No features or targets generated")
                return
                
            self.log_test("Pipeline Data Preparation", True,
                         f"Features: {X.shape}, Target: {len(y)}")
            
            # Test data splitting | æ¸¬è©¦æ•¸æ“šåˆ†å‰²
            splits = pipeline.prepare_data_splits(X, y, test_size=0.2, val_size=0.2)
            if splits and len(splits) == 3:
                self.log_test("Pipeline Data Splitting", True, "Train/Val/Test splits created")
            else:
                self.log_test("Pipeline Data Splitting", False, "Data splitting failed")
            
        except Exception as e:
            self.log_test("Training Pipeline", False, f"Error: {str(e)}")
            traceback.print_exc()
    
    def test_performance_metrics(self):
        """Test performance metrics | æ¸¬è©¦æ€§èƒ½æŒ‡æ¨™"""
        print(f"\n{Colors.BOLD}7. Performance Metrics Tests | æ€§èƒ½æŒ‡æ¨™æ¸¬è©¦{Colors.END}")
        
        try:
            # Test import | æ¸¬è©¦å°å…¥
            from evaluation.performance_metrics import PerformanceMetrics
            self.log_test("Performance Metrics Import", True)
            
            # Create sample predictions and actual values | å‰µå»ºæ¨£æœ¬é æ¸¬å’Œå¯¦éš›å€¼
            np.random.seed(42)  # For reproducible results | ç‚ºå¯é‡ç¾çµæœ
            n_samples = 100
            
            # Generate sample data | ç”Ÿæˆæ¨£æœ¬æ•¸æ“š
            y_true = np.random.choice([0, 1], size=n_samples, p=[0.6, 0.4])
            y_pred = np.random.choice([0, 1], size=n_samples, p=[0.55, 0.45])
            y_pred_proba = np.random.random(n_samples)
            
            # Create sample returns for trading metrics | ç‚ºäº¤æ˜“æŒ‡æ¨™å‰µå»ºæ¨£æœ¬å›å ±
            returns = np.random.normal(0.001, 0.02, n_samples)
            
            # Test performance metrics | æ¸¬è©¦æ€§èƒ½æŒ‡æ¨™
            metrics = PerformanceMetrics()
            
            # Test classification metrics | æ¸¬è©¦åˆ†é¡æŒ‡æ¨™
            classification_results = metrics.calculate_classification_metrics(y_true, y_pred, y_pred_proba)
            if classification_results and 'accuracy' in classification_results:
                self.log_test("Classification Metrics", True, 
                             f"Accuracy: {classification_results['accuracy']:.3f}")
            else:
                self.log_test("Classification Metrics", False, "Classification metrics failed")
            
            # Test trading metrics | æ¸¬è©¦äº¤æ˜“æŒ‡æ¨™
            trading_results = metrics.calculate_trading_metrics(returns, y_pred)
            if trading_results and 'total_return' in trading_results:
                self.log_test("Trading Metrics", True, 
                             f"Total return: {trading_results['total_return']:.3f}")
            else:
                self.log_test("Trading Metrics", False, "Trading metrics failed")
            
            # Test directional accuracy | æ¸¬è©¦æ–¹å‘æº–ç¢ºåº¦
            direction_accuracy = metrics.calculate_directional_accuracy(y_true, y_pred)
            if direction_accuracy is not None:
                self.log_test("Directional Accuracy", True, f"Direction accuracy: {direction_accuracy:.3f}")
            else:
                self.log_test("Directional Accuracy", False, "Direction accuracy calculation failed")
            
        except Exception as e:
            self.log_test("Performance Metrics", False, f"Error: {str(e)}")
    
    def test_model_management(self):
        """Test model management system | æ¸¬è©¦æ¨¡å‹ç®¡ç†ç³»çµ±"""
        print(f"\n{Colors.BOLD}8. Model Management Tests | æ¨¡å‹ç®¡ç†æ¸¬è©¦{Colors.END}")
        
        try:
            # Test import | æ¸¬è©¦å°å…¥
            from services.model_manager import ModelManager
            self.log_test("Model Manager Import", True)
            
            # Test manager creation | æ¸¬è©¦ç®¡ç†å™¨å‰µå»º
            manager = ModelManager()
            self.log_test("Model Manager Creation", True)
            
            # Test model versioning | æ¸¬è©¦æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶
            version_info = manager.get_version_info()
            if version_info and 'version' in version_info:
                self.log_test("Model Versioning", True, f"Version: {version_info['version']}")
            else:
                self.log_test("Model Versioning", False, "Version info not available")
            
            # Test model registry access | æ¸¬è©¦æ¨¡å‹è¨»å†Šè¡¨è¨ªå•
            available_models = manager.list_available_models()
            if isinstance(available_models, list):
                self.log_test("Model Registry Access", True, f"Found {len(available_models)} model types")
            else:
                self.log_test("Model Registry Access", False, "Registry access failed")
            
        except Exception as e:
            self.log_test("Model Management", False, f"Error: {str(e)}")
    
    def _calculate_rsi(self, prices, window=14):
        """Calculate RSI for testing | è¨ˆç®—RSIç”¨æ–¼æ¸¬è©¦"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def generate_report(self):
        """Generate final test report | ç”Ÿæˆæœ€çµ‚æ¸¬è©¦å ±å‘Š"""
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        print(f"\n{Colors.BOLD}{Colors.BLUE}ğŸ¤– AIFX Phase 2 Test Report | AIFXç¬¬äºŒéšæ®µæ¸¬è©¦å ±å‘Š{Colors.END}")
        print("="*80)
        
        # Summary statistics | æ‘˜è¦çµ±è¨ˆ
        pass_rate = (self.passed_tests / self.total_tests * 100) if self.total_tests > 0 else 0
        
        print(f"{Colors.BOLD}Test Summary | æ¸¬è©¦æ‘˜è¦:{Colors.END}")
        print(f"  Total Tests: {self.total_tests}")
        print(f"  Passed: {Colors.GREEN}{self.passed_tests}{Colors.END}")
        print(f"  Failed: {Colors.RED}{self.failed_tests}{Colors.END}")
        print(f"  Pass Rate: {Colors.GREEN if pass_rate >= 80 else Colors.YELLOW}{pass_rate:.1f}%{Colors.END}")
        print(f"  Duration: {duration:.2f}s")
        
        if self.warnings:
            print(f"\n{Colors.YELLOW}âš ï¸ Warnings ({len(self.warnings)}):{Colors.END}")
            for warning in self.warnings[:5]:  # Show first 5 warnings | é¡¯ç¤ºå‰5å€‹è­¦å‘Š
                print(f"  â€¢ {warning}")
            if len(self.warnings) > 5:
                print(f"  ... and {len(self.warnings) - 5} more")
        
        # Overall assessment | ç¸½é«”è©•ä¼°
        print(f"\n{Colors.BOLD}Overall Assessment | ç¸½é«”è©•ä¼°:{Colors.END}")
        
        if pass_rate >= 90:
            status = f"{Colors.GREEN}âœ… EXCELLENT{Colors.END}"
            message = "Phase 2 AI models are fully functional | ç¬¬äºŒéšæ®µAIæ¨¡å‹å®Œå…¨æ­£å¸¸"
        elif pass_rate >= 80:
            status = f"{Colors.YELLOW}âš ï¸ GOOD{Colors.END}"
            message = "Phase 2 AI models are mostly functional with minor issues | ç¬¬äºŒéšæ®µAIæ¨¡å‹å¤§éƒ¨åˆ†æ­£å¸¸ï¼Œæœ‰è¼•å¾®å•é¡Œ"
        elif pass_rate >= 60:
            status = f"{Colors.YELLOW}âš ï¸ ACCEPTABLE{Colors.END}"
            message = "Phase 2 AI models have some issues that should be addressed | ç¬¬äºŒéšæ®µAIæ¨¡å‹æœ‰ä¸€äº›æ‡‰è©²è§£æ±ºçš„å•é¡Œ"
        else:
            status = f"{Colors.RED}âŒ NEEDS WORK{Colors.END}"
            message = "Phase 2 AI models have significant issues | ç¬¬äºŒéšæ®µAIæ¨¡å‹æœ‰é‡å¤§å•é¡Œ"
        
        print(f"  Status: {status}")
        print(f"  {message}")
        
        # Next steps | ä¸‹ä¸€æ­¥
        print(f"\n{Colors.BOLD}Next Steps | ä¸‹ä¸€æ­¥:{Colors.END}")
        if self.failed_tests > 0:
            print(f"  1. Review and fix {self.failed_tests} failed tests | æª¢æŸ¥ä¸¦ä¿®å¾©{self.failed_tests}å€‹å¤±æ•—çš„æ¸¬è©¦")
        if self.warnings:
            print(f"  2. Address {len(self.warnings)} warnings if needed | å¦‚æœ‰éœ€è¦ï¼Œè™•ç†{len(self.warnings)}å€‹è­¦å‘Š")
        
        if pass_rate >= 80:
            print(f"  3. {Colors.GREEN}Ready for Phase 1-2 Integration Testing{Colors.END}")
            print(f"     {Colors.GREEN}æº–å‚™é€²è¡Œç¬¬ä¸€éšæ®µ-ç¬¬äºŒéšæ®µæ•´åˆæ¸¬è©¦{Colors.END}")
        else:
            print(f"  3. {Colors.YELLOW}Recommend fixing issues before integration{Colors.END}")
            print(f"     {Colors.YELLOW}å»ºè­°åœ¨æ•´åˆå‰ä¿®å¾©å•é¡Œ{Colors.END}")
        
        return pass_rate >= 80

def main():
    """Main test execution | ä¸»è¦æ¸¬è©¦åŸ·è¡Œ"""
    tester = Phase2Tester()
    
    try:
        # Run all test suites | é‹è¡Œæ‰€æœ‰æ¸¬è©¦å¥—ä»¶
        tester.test_ai_dependencies()
        tester.test_base_model_framework()
        tester.test_xgboost_model()
        tester.test_random_forest_model()
        tester.test_lstm_model()
        tester.test_training_pipeline()
        tester.test_performance_metrics()
        tester.test_model_management()
        
        # Generate final report | ç”Ÿæˆæœ€çµ‚å ±å‘Š
        success = tester.generate_report()
        
        # Return appropriate exit code | è¿”å›é©ç•¶çš„é€€å‡ºä»£ç¢¼
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print(f"\n{Colors.YELLOW}Test interrupted by user | ç”¨æˆ¶ä¸­æ–·æ¸¬è©¦{Colors.END}")
        sys.exit(1)
    except Exception as e:
        print(f"\n{Colors.RED}Fatal error during testing | æ¸¬è©¦æœŸé–“è‡´å‘½éŒ¯èª¤: {str(e)}{Colors.END}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()