"""
Durability Test Suite for Phase 3.1.1 Signal Combination System | ç¬¬ä¸‰éšæ®µ3.1.1ä¿¡è™Ÿçµ„åˆç³»çµ±è€ä¹…æ€§æ¸¬è©¦

Advanced durability and stress testing for the complete signal combination framework:
- Memory leak detection and resource management
- High-volume signal processing capability
- Edge case resilience and error recovery
- Long-running operation stability
- Concurrent operation safety
- Performance degradation monitoring

é«˜ç´šè€ä¹…æ€§å’Œå£“åŠ›æ¸¬è©¦ï¼Œé‡å°å®Œæ•´çš„ä¿¡è™Ÿçµ„åˆæ¡†æ¶ï¼š
- è¨˜æ†¶é«”æ´©æ¼æª¢æ¸¬å’Œè³‡æºç®¡ç†
- é«˜å®¹é‡ä¿¡è™Ÿè™•ç†èƒ½åŠ›
- é‚Šç·£æƒ…æ³éŸŒæ€§å’ŒéŒ¯èª¤æ¢å¾©
- é•·æ™‚é–“é‹è¡Œç©©å®šæ€§
- ä¸¦ç™¼æ“ä½œå®‰å…¨æ€§
- æ€§èƒ½é™ç´šç›£æ§
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append('src/main/python')

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import logging
import time
import gc
import tracemalloc
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any
import unittest
from unittest.mock import Mock, patch
import warnings

# Import the signal combination system components
from core.signal_combiner import BaseSignalCombiner, TradingSignal, SignalType, SignalAggregator
from core.confidence_scorer import AdvancedConfidenceScorer, ConfidenceComponents
from core.weight_optimizer import DynamicWeightOptimizer, WeightOptimizationConfig, OptimizationMethod
from core.ai_signal_combiner import AISignalCombiner

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Suppress warnings for cleaner test output
warnings.filterwarnings("ignore", category=UserWarning)


class SignalCombinationDurabilityTest:
    """
    Comprehensive durability test suite for signal combination system
    ä¿¡è™Ÿçµ„åˆç³»çµ±ç¶œåˆè€ä¹…æ€§æ¸¬è©¦å¥—ä»¶
    """
    
    def __init__(self):
        """Initialize durability test environment | åˆå§‹åŒ–è€ä¹…æ€§æ¸¬è©¦ç’°å¢ƒ"""
        logger.info("Initializing Signal Combination Durability Test Suite...")
        
        # Test configuration | æ¸¬è©¦é…ç½®
        self.test_config = {
            'high_volume_signal_count': 10000,
            'stress_test_duration': 60,  # seconds
            'memory_threshold_mb': 50,
            'concurrent_threads': 10,
            'performance_iterations': 1000
        }
        
        # Initialize components | åˆå§‹åŒ–çµ„ä»¶
        self.setup_test_components()
        
        # Performance tracking | æ€§èƒ½è¿½è¹¤
        self.performance_metrics = {
            'memory_usage': [],
            'processing_times': [],
            'error_count': 0,
            'successful_operations': 0
        }
        
    def setup_test_components(self):
        """Setup test components and mock data | è¨­ç½®æ¸¬è©¦çµ„ä»¶å’Œæ¨¡æ“¬æ•¸æ“š"""
        # Mock AI models for testing | ç”¨æ–¼æ¸¬è©¦çš„æ¨¡æ“¬AIæ¨¡å‹
        self.mock_models = {
            'XGBoost': self.create_mock_model('XGBoost'),
            'RandomForest': self.create_mock_model('RandomForest'),
            'LSTM': self.create_mock_model('LSTM')
        }
        
        # Initialize test components | åˆå§‹åŒ–æ¸¬è©¦çµ„ä»¶
        self.ai_combiner = AISignalCombiner(self.mock_models)
        self.confidence_scorer = AdvancedConfidenceScorer()
        self.weight_optimizer = DynamicWeightOptimizer()
        self.signal_aggregator = SignalAggregator()
        
        # Create large test datasets | å‰µå»ºå¤§å‹æ¸¬è©¦æ•¸æ“šé›†
        self.large_market_data = self.create_large_market_dataset(10000)  # 10K records
        
    def create_mock_model(self, model_name: str) -> Mock:
        """Create mock AI model with consistent behavior | å‰µå»ºè¡Œç‚ºä¸€è‡´çš„æ¨¡æ“¬AIæ¨¡å‹"""
        mock_model = Mock()
        mock_model.model_name = model_name
        mock_model.version = "1.0"
        mock_model.is_trained = True
        mock_model.metadata = {
            'training_samples': 10000,
            'performance_metrics': {'accuracy': 0.65 + np.random.random() * 0.1}
        }
        
        # Consistent prediction patterns | ä¸€è‡´çš„é æ¸¬æ¨¡å¼
        np.random.seed(hash(model_name) % 2**31)
        mock_model.predict.return_value = np.array([np.random.random() * 2 - 1])
        proba = np.random.dirichlet([1, 1])
        mock_model.predict_proba.return_value = np.array([proba])
        
        return mock_model
    
    def create_large_market_dataset(self, size: int) -> pd.DataFrame:
        """Create large synthetic market dataset | å‰µå»ºå¤§å‹åˆæˆå¸‚å ´æ•¸æ“šé›†"""
        dates = pd.date_range(start='2020-01-01', periods=size, freq='1H')
        
        # Generate realistic price movements | ç”ŸæˆçœŸå¯¦çš„åƒ¹æ ¼è®Šå‹•
        np.random.seed(42)
        initial_price = 1.1000
        returns = np.random.normal(0.0001, 0.002, size)
        prices = np.zeros(size)
        prices[0] = initial_price
        
        for i in range(1, size):
            prices[i] = prices[i-1] * (1 + returns[i])
        
        market_data = pd.DataFrame({
            'Open': prices * (1 + np.random.normal(0, 0.0001, size)),
            'High': prices * (1 + np.abs(np.random.normal(0, 0.0005, size))),
            'Low': prices * (1 - np.abs(np.random.normal(0, 0.0005, size))),
            'Close': prices,
            'Volume': np.random.randint(1000, 10000, size)
        }, index=dates)
        
        return market_data
    
    def create_high_volume_signals(self, count: int) -> List[TradingSignal]:
        """Create high volume of test signals | å‰µå»ºå¤§é‡æ¸¬è©¦ä¿¡è™Ÿ"""
        signals = []
        current_time = datetime.now()
        
        for i in range(count):
            signal_type = np.random.choice([SignalType.BUY, SignalType.SELL, SignalType.HOLD])
            strength = np.random.random()
            confidence = np.random.random()
            
            # Vary sources to test different scenarios | è®ŠåŒ–ä¾†æºä»¥æ¸¬è©¦ä¸åŒæƒ…æ³
            sources = ['AI_XGBoost', 'AI_RandomForest', 'AI_LSTM', 'Tech_MA', 'Tech_RSI', 'Tech_MACD']
            source = np.random.choice(sources)
            
            timestamp = current_time - timedelta(seconds=np.random.randint(0, 3600))
            
            signal = TradingSignal(
                signal_type=signal_type,
                strength=strength,
                confidence=confidence,
                source=source,
                timestamp=timestamp,
                metadata={'batch_id': i // 1000, 'test_signal': True}
            )
            
            signals.append(signal)
        
        return signals
    
    def monitor_memory_usage(self) -> float:
        """Monitor current memory usage in MB | ç›£æ§ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            return memory_mb
        except ImportError:
            # Fallback to tracemalloc if psutil not available | å¦‚æœpsutilä¸å¯ç”¨å‰‡å›é€€åˆ°tracemalloc
            current, peak = tracemalloc.get_traced_memory()
            return current / 1024 / 1024
    
    def test_1_memory_leak_detection(self) -> Dict[str, Any]:
        """Test 1: Memory leak detection during continuous operation | æ¸¬è©¦1ï¼šé€£çºŒé‹è¡ŒæœŸé–“çš„è¨˜æ†¶é«”æ´©æ¼æª¢æ¸¬"""
        logger.info("ğŸ§ª Durability Test 1: Memory leak detection")
        
        # Start memory tracking | é–‹å§‹è¨˜æ†¶é«”è¿½è¹¤
        tracemalloc.start()
        initial_memory = self.monitor_memory_usage()
        
        test_results = {
            'status': 'RUNNING',
            'initial_memory_mb': initial_memory,
            'peak_memory_mb': initial_memory,
            'final_memory_mb': 0,
            'memory_growth_mb': 0,
            'iterations': 100,
            'leak_detected': False
        }
        
        try:
            # Perform repeated operations | åŸ·è¡Œé‡è¤‡æ“ä½œ
            for iteration in range(test_results['iterations']):
                # Create and process signals | å‰µå»ºå’Œè™•ç†ä¿¡è™Ÿ
                test_signals = self.create_high_volume_signals(100)
                
                # Process through all components | é€šéæ‰€æœ‰çµ„ä»¶è™•ç†
                for signal in test_signals:
                    self.signal_aggregator.add_signal(signal)
                
                # Combine signals | çµ„åˆä¿¡è™Ÿ
                combined = self.ai_combiner.combine_signals(test_signals[:10])  # Subset for efficiency
                
                # Calculate confidence | è¨ˆç®—ä¿¡å¿ƒ
                confidence = self.confidence_scorer.calculate_comprehensive_confidence(
                    test_signals[:5], 
                    self.large_market_data.tail(50)
                )
                
                # Optimize weights | å„ªåŒ–æ¬Šé‡
                sources = list(set(s.source for s in test_signals[:10]))
                weights = self.weight_optimizer.optimize_weights(sources)
                
                # Clear aggregator periodically | å®šæœŸæ¸…ç†èšåˆå™¨
                if iteration % 10 == 0:
                    self.signal_aggregator.clear()
                    gc.collect()  # Force garbage collection
                
                # Monitor memory | ç›£æ§è¨˜æ†¶é«”
                current_memory = self.monitor_memory_usage()
                test_results['peak_memory_mb'] = max(test_results['peak_memory_mb'], current_memory)
                
                # Check for concerning memory growth | æª¢æŸ¥ä»¤äººæ“”æ†‚çš„è¨˜æ†¶é«”å¢é•·
                if current_memory > initial_memory + self.test_config['memory_threshold_mb']:
                    test_results['leak_detected'] = True
                    logger.warning(f"Potential memory leak detected at iteration {iteration}: "
                                 f"{current_memory:.1f}MB (started at {initial_memory:.1f}MB)")
            
            test_results['final_memory_mb'] = self.monitor_memory_usage()
            test_results['memory_growth_mb'] = test_results['final_memory_mb'] - initial_memory
            test_results['status'] = 'PASSED' if not test_results['leak_detected'] else 'WARNING'
            
        except Exception as e:
            test_results['status'] = 'FAILED'
            test_results['error'] = str(e)
        
        finally:
            tracemalloc.stop()
        
        logger.info(f"âœ… Memory Test Result: {test_results['status']} - "
                   f"Growth: {test_results['memory_growth_mb']:.1f}MB")
        
        return test_results
    
    def test_2_high_volume_processing(self) -> Dict[str, Any]:
        """Test 2: High volume signal processing capability | æ¸¬è©¦2ï¼šé«˜å®¹é‡ä¿¡è™Ÿè™•ç†èƒ½åŠ›"""
        logger.info("ğŸ§ª Durability Test 2: High volume signal processing")
        
        test_results = {
            'status': 'RUNNING',
            'total_signals': self.test_config['high_volume_signal_count'],
            'processing_time': 0,
            'signals_per_second': 0,
            'memory_efficiency': True,
            'errors_encountered': 0
        }
        
        try:
            # Create high volume of signals | å‰µå»ºå¤§é‡ä¿¡è™Ÿ
            start_time = time.time()
            high_volume_signals = self.create_high_volume_signals(test_results['total_signals'])
            creation_time = time.time() - start_time
            
            logger.info(f"Created {len(high_volume_signals)} signals in {creation_time:.2f}s")
            
            # Process signals in batches | æ‰¹æ¬¡è™•ç†ä¿¡è™Ÿ
            batch_size = 1000
            processing_start = time.time()
            
            for i in range(0, len(high_volume_signals), batch_size):
                batch = high_volume_signals[i:i+batch_size]
                
                try:
                    # Process batch through signal aggregator | é€šéä¿¡è™Ÿèšåˆå™¨è™•ç†æ‰¹æ¬¡
                    for signal in batch:
                        self.signal_aggregator.add_signal(signal)
                    
                    # Get batch summary | ç²å–æ‰¹æ¬¡æ‘˜è¦
                    summary = self.signal_aggregator.get_summary()
                    
                    # Test confidence scoring on subset | åœ¨å­é›†ä¸Šæ¸¬è©¦ä¿¡å¿ƒè©•åˆ†
                    sample_signals = batch[:10]
                    confidence_components = self.confidence_scorer.calculate_comprehensive_confidence(
                        sample_signals,
                        self.large_market_data.tail(100)
                    )
                    
                    # Test weight optimization | æ¸¬è©¦æ¬Šé‡å„ªåŒ–
                    sources = list(set(s.source for s in sample_signals))
                    weights = self.weight_optimizer.optimize_weights(sources)
                    
                    # Clear aggregator for next batch | ç‚ºä¸‹ä¸€æ‰¹æ¬¡æ¸…ç†èšåˆå™¨
                    self.signal_aggregator.clear()
                    
                except Exception as e:
                    test_results['errors_encountered'] += 1
                    if test_results['errors_encountered'] > 10:  # Fail if too many errors
                        raise e
            
            processing_time = time.time() - processing_start
            test_results['processing_time'] = processing_time
            test_results['signals_per_second'] = test_results['total_signals'] / processing_time
            
            # Check memory efficiency | æª¢æŸ¥è¨˜æ†¶é«”æ•ˆç‡
            final_memory = self.monitor_memory_usage()
            test_results['memory_efficiency'] = final_memory < 100  # Less than 100MB
            
            test_results['status'] = 'PASSED' if test_results['errors_encountered'] < 5 else 'WARNING'
            
        except Exception as e:
            test_results['status'] = 'FAILED'
            test_results['error'] = str(e)
        
        logger.info(f"âœ… High Volume Test Result: {test_results['status']} - "
                   f"{test_results['signals_per_second']:.0f} signals/sec")
        
        return test_results
    
    def test_3_concurrent_operation_safety(self) -> Dict[str, Any]:
        """Test 3: Concurrent operation safety and thread safety | æ¸¬è©¦3ï¼šä¸¦ç™¼æ“ä½œå®‰å…¨æ€§å’Œç·šç¨‹å®‰å…¨æ€§"""
        logger.info("ğŸ§ª Durability Test 3: Concurrent operation safety")
        
        test_results = {
            'status': 'RUNNING',
            'concurrent_threads': self.test_config['concurrent_threads'],
            'operations_per_thread': 100,
            'total_operations': 0,
            'successful_operations': 0,
            'thread_errors': 0,
            'race_conditions_detected': 0
        }
        
        def worker_thread(thread_id: int) -> Dict[str, Any]:
            """Worker thread for concurrent testing | ä¸¦ç™¼æ¸¬è©¦çš„å·¥ä½œç·šç¨‹"""
            thread_results = {
                'thread_id': thread_id,
                'operations': 0,
                'errors': 0,
                'processing_time': 0
            }
            
            start_time = time.time()
            
            try:
                # Create separate components for this thread | ç‚ºæ­¤ç·šç¨‹å‰µå»ºå–®ç¨çš„çµ„ä»¶
                thread_aggregator = SignalAggregator()
                thread_confidence_scorer = AdvancedConfidenceScorer()
                
                for i in range(test_results['operations_per_thread']):
                    # Create signals for this thread | ç‚ºæ­¤ç·šç¨‹å‰µå»ºä¿¡è™Ÿ
                    thread_signals = self.create_high_volume_signals(10)
                    
                    # Process signals | è™•ç†ä¿¡è™Ÿ
                    for signal in thread_signals:
                        thread_aggregator.add_signal(signal)
                    
                    # Test confidence calculation | æ¸¬è©¦ä¿¡å¿ƒè¨ˆç®—
                    confidence = thread_confidence_scorer.calculate_comprehensive_confidence(
                        thread_signals[:5],
                        self.large_market_data.tail(50)
                    )
                    
                    # Test AI combiner (using shared instance - this tests thread safety)
                    # æ¸¬è©¦AIçµ„åˆå™¨ï¼ˆä½¿ç”¨å…±äº«å¯¦ä¾‹ - é€™æ¸¬è©¦ç·šç¨‹å®‰å…¨æ€§ï¼‰
                    combined = self.ai_combiner.combine_signals(thread_signals[:3])
                    
                    thread_results['operations'] += 1
                    
                    # Small delay to increase chance of race conditions | å°å»¶é²ä»¥å¢åŠ ç«¶æ…‹æ¢ä»¶çš„æ©Ÿæœƒ
                    time.sleep(0.001)
                
            except Exception as e:
                thread_results['errors'] += 1
                logger.warning(f"Thread {thread_id} error: {e}")
            
            thread_results['processing_time'] = time.time() - start_time
            return thread_results
        
        try:
            # Launch concurrent threads | å•Ÿå‹•ä¸¦ç™¼ç·šç¨‹
            with ThreadPoolExecutor(max_workers=test_results['concurrent_threads']) as executor:
                # Submit all worker threads | æäº¤æ‰€æœ‰å·¥ä½œç·šç¨‹
                futures = [
                    executor.submit(worker_thread, i) 
                    for i in range(test_results['concurrent_threads'])
                ]
                
                # Collect results | æ”¶é›†çµæœ
                thread_results = []
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=30)  # 30 second timeout per thread
                        thread_results.append(result)
                        test_results['successful_operations'] += result['operations']
                        test_results['thread_errors'] += result['errors']
                    except Exception as e:
                        test_results['thread_errors'] += 1
                        logger.error(f"Thread execution failed: {e}")
            
            test_results['total_operations'] = test_results['successful_operations'] + test_results['thread_errors']
            
            # Analyze results for race conditions | åˆ†æçµæœä¸­çš„ç«¶æ…‹æ¢ä»¶
            expected_operations = test_results['concurrent_threads'] * test_results['operations_per_thread']
            if test_results['total_operations'] != expected_operations:
                test_results['race_conditions_detected'] = expected_operations - test_results['total_operations']
            
            # Determine test status | ç¢ºå®šæ¸¬è©¦ç‹€æ…‹
            if test_results['thread_errors'] == 0 and test_results['race_conditions_detected'] == 0:
                test_results['status'] = 'PASSED'
            elif test_results['thread_errors'] < 5:
                test_results['status'] = 'WARNING'
            else:
                test_results['status'] = 'FAILED'
            
        except Exception as e:
            test_results['status'] = 'FAILED'
            test_results['error'] = str(e)
        
        logger.info(f"âœ… Concurrent Test Result: {test_results['status']} - "
                   f"{test_results['successful_operations']}/{test_results['total_operations']} operations")
        
        return test_results
    
    def test_4_long_running_stability(self) -> Dict[str, Any]:
        """Test 4: Long-running operation stability | æ¸¬è©¦4ï¼šé•·æ™‚é–“é‹è¡Œç©©å®šæ€§"""
        logger.info("ğŸ§ª Durability Test 4: Long-running stability")
        
        test_results = {
            'status': 'RUNNING',
            'duration_seconds': self.test_config['stress_test_duration'],
            'operations_completed': 0,
            'average_processing_time': 0,
            'performance_degradation': False,
            'stability_issues': 0,
            'memory_stable': True
        }
        
        start_time = time.time()
        processing_times = []
        memory_readings = []
        
        try:
            operation_count = 0
            
            while (time.time() - start_time) < test_results['duration_seconds']:
                operation_start = time.time()
                
                try:
                    # Simulate realistic trading signal processing | æ¨¡æ“¬çœŸå¯¦çš„äº¤æ˜“ä¿¡è™Ÿè™•ç†
                    test_signals = self.create_high_volume_signals(50)
                    
                    # Process through complete pipeline | é€šéå®Œæ•´ç®¡é“è™•ç†
                    # 1. Aggregation | 1. èšåˆ
                    for signal in test_signals:
                        self.signal_aggregator.add_signal(signal)
                    
                    # 2. AI combination | 2. AIçµ„åˆ
                    ai_signals = test_signals[:10]  # Subset for AI processing
                    combined_signal = self.ai_combiner.combine_signals(ai_signals)
                    
                    # 3. Confidence scoring | 3. ä¿¡å¿ƒè©•åˆ†
                    confidence = self.confidence_scorer.calculate_comprehensive_confidence(
                        ai_signals,
                        self.large_market_data.tail(100)
                    )
                    
                    # 4. Weight optimization | 4. æ¬Šé‡å„ªåŒ–
                    sources = list(set(s.source for s in test_signals[:10]))
                    if sources:
                        weights = self.weight_optimizer.optimize_weights(sources)
                    
                    # Track performance | è¿½è¹¤æ€§èƒ½
                    operation_time = time.time() - operation_start
                    processing_times.append(operation_time)
                    
                    # Monitor memory periodically | å®šæœŸç›£æ§è¨˜æ†¶é«”
                    if operation_count % 10 == 0:
                        memory_mb = self.monitor_memory_usage()
                        memory_readings.append(memory_mb)
                    
                    # Clear aggregator periodically to prevent memory buildup | å®šæœŸæ¸…ç†èšåˆå™¨ä»¥é˜²æ­¢è¨˜æ†¶é«”ç©ç´¯
                    if operation_count % 20 == 0:
                        self.signal_aggregator.clear()
                        gc.collect()
                    
                    operation_count += 1
                    
                except Exception as e:
                    test_results['stability_issues'] += 1
                    logger.warning(f"Stability issue in operation {operation_count}: {e}")
                    
                    # Fail if too many stability issues | å¦‚æœç©©å®šæ€§å•é¡Œå¤ªå¤šå‰‡å¤±æ•—
                    if test_results['stability_issues'] > 10:
                        raise e
            
            test_results['operations_completed'] = operation_count
            
            # Analyze performance metrics | åˆ†ææ€§èƒ½æŒ‡æ¨™
            if processing_times:
                test_results['average_processing_time'] = np.mean(processing_times)
                
                # Check for performance degradation | æª¢æŸ¥æ€§èƒ½é™ç´š
                first_half = processing_times[:len(processing_times)//2]
                second_half = processing_times[len(processing_times)//2:]
                
                if len(first_half) > 10 and len(second_half) > 10:
                    first_half_avg = np.mean(first_half)
                    second_half_avg = np.mean(second_half)
                    
                    # Performance degradation if second half is 50% slower | å¦‚æœå¾ŒåŠæ®µæ…¢50%å‰‡æ€§èƒ½é™ç´š
                    if second_half_avg > first_half_avg * 1.5:
                        test_results['performance_degradation'] = True
            
            # Analyze memory stability | åˆ†æè¨˜æ†¶é«”ç©©å®šæ€§
            if len(memory_readings) > 2:
                memory_growth = memory_readings[-1] - memory_readings[0]
                if memory_growth > 20:  # More than 20MB growth is concerning | è¶…é20MBå¢é•·ä»¤äººæ“”æ†‚
                    test_results['memory_stable'] = False
            
            # Determine overall status | ç¢ºå®šæ•´é«”ç‹€æ…‹
            if (test_results['stability_issues'] == 0 and 
                not test_results['performance_degradation'] and 
                test_results['memory_stable']):
                test_results['status'] = 'PASSED'
            elif test_results['stability_issues'] < 5:
                test_results['status'] = 'WARNING'
            else:
                test_results['status'] = 'FAILED'
                
        except Exception as e:
            test_results['status'] = 'FAILED'
            test_results['error'] = str(e)
        
        actual_duration = time.time() - start_time
        logger.info(f"âœ… Long-running Test Result: {test_results['status']} - "
                   f"{test_results['operations_completed']} ops in {actual_duration:.1f}s")
        
        return test_results
    
    def test_5_extreme_edge_cases(self) -> Dict[str, Any]:
        """Test 5: Extreme edge cases and error recovery | æ¸¬è©¦5ï¼šæ¥µç«¯é‚Šç·£æƒ…æ³å’ŒéŒ¯èª¤æ¢å¾©"""
        logger.info("ğŸ§ª Durability Test 5: Extreme edge cases")
        
        test_results = {
            'status': 'RUNNING',
            'edge_cases_tested': 0,
            'edge_cases_handled': 0,
            'critical_failures': 0,
            'recovery_successful': 0
        }
        
        edge_test_cases = [
            self._test_empty_data_handling,
            self._test_corrupted_signals,
            self._test_extreme_values,
            self._test_null_data_handling,
            self._test_circular_dependencies,
            self._test_resource_exhaustion_simulation,
            self._test_timing_edge_cases,
            self._test_concurrent_modifications
        ]
        
        try:
            for i, edge_test in enumerate(edge_test_cases):
                test_results['edge_cases_tested'] += 1
                
                try:
                    logger.info(f"Running edge case test {i+1}/{len(edge_test_cases)}: {edge_test.__name__}")
                    result = edge_test()
                    
                    if result.get('handled', False):
                        test_results['edge_cases_handled'] += 1
                    
                    if result.get('recovered', False):
                        test_results['recovery_successful'] += 1
                    
                    if result.get('critical_failure', False):
                        test_results['critical_failures'] += 1
                        
                except Exception as e:
                    test_results['critical_failures'] += 1
                    logger.error(f"Critical failure in edge case {i+1}: {e}")
            
            # Determine test status | ç¢ºå®šæ¸¬è©¦ç‹€æ…‹
            success_rate = test_results['edge_cases_handled'] / test_results['edge_cases_tested']
            
            if success_rate >= 0.9 and test_results['critical_failures'] == 0:
                test_results['status'] = 'PASSED'
            elif success_rate >= 0.7:
                test_results['status'] = 'WARNING'
            else:
                test_results['status'] = 'FAILED'
                
        except Exception as e:
            test_results['status'] = 'FAILED'
            test_results['error'] = str(e)
        
        logger.info(f"âœ… Edge Cases Test Result: {test_results['status']} - "
                   f"{test_results['edge_cases_handled']}/{test_results['edge_cases_tested']} handled")
        
        return test_results
    
    def _test_empty_data_handling(self) -> Dict[str, Any]:
        """Test empty data handling | æ¸¬è©¦ç©ºæ•¸æ“šè™•ç†"""
        try:
            # Test with empty signals | ç”¨ç©ºä¿¡è™Ÿæ¸¬è©¦
            result = self.ai_combiner.combine_signals([])
            confidence = self.confidence_scorer.calculate_comprehensive_confidence([])
            weights = self.weight_optimizer.optimize_weights([])
            
            return {'handled': True, 'recovered': True, 'critical_failure': False}
        except Exception:
            return {'handled': False, 'recovered': False, 'critical_failure': True}
    
    def _test_corrupted_signals(self) -> Dict[str, Any]:
        """Test corrupted signal handling | æ¸¬è©¦æå£ä¿¡è™Ÿè™•ç†"""
        try:
            # Create signals with corrupted data | å‰µå»ºå¸¶æœ‰æå£æ•¸æ“šçš„ä¿¡è™Ÿ
            corrupted_signal = TradingSignal(
                SignalType.BUY, float('inf'), float('nan'), None
            )
            
            result = self.ai_combiner.combine_signals([corrupted_signal])
            return {'handled': True, 'recovered': True, 'critical_failure': False}
        except Exception:
            return {'handled': False, 'recovered': False, 'critical_failure': True}
    
    def _test_extreme_values(self) -> Dict[str, Any]:
        """Test extreme value handling | æ¸¬è©¦æ¥µå€¼è™•ç†"""
        try:
            # Test with extreme values | ç”¨æ¥µå€¼æ¸¬è©¦
            extreme_signals = [
                TradingSignal(SignalType.BUY, 1e10, 1e10, "extreme_source"),
                TradingSignal(SignalType.SELL, -1e10, -1e10, "extreme_source2")
            ]
            
            result = self.ai_combiner.combine_signals(extreme_signals)
            return {'handled': True, 'recovered': True, 'critical_failure': False}
        except Exception:
            return {'handled': False, 'recovered': False, 'critical_failure': True}
    
    def _test_null_data_handling(self) -> Dict[str, Any]:
        """Test null data handling | æ¸¬è©¦ç©ºæ•¸æ“šè™•ç†"""
        try:
            # Test with None values | ç”¨Noneå€¼æ¸¬è©¦
            confidence = self.confidence_scorer.calculate_comprehensive_confidence(
                None, None, None
            )
            return {'handled': True, 'recovered': True, 'critical_failure': False}
        except Exception:
            return {'handled': False, 'recovered': False, 'critical_failure': True}
    
    def _test_circular_dependencies(self) -> Dict[str, Any]:
        """Test circular dependency handling | æ¸¬è©¦å¾ªç’°ä¾è³´è™•ç†"""
        try:
            # This is more of a design test - ensure no circular references | é€™æ›´åƒæ˜¯è¨­è¨ˆæ¸¬è©¦ - ç¢ºä¿æ²’æœ‰å¾ªç’°å¼•ç”¨
            # Create complex object relationships | å‰µå»ºè¤‡é›œçš„å°è±¡é—œä¿‚
            aggregator1 = SignalAggregator()
            aggregator2 = SignalAggregator()
            
            # Test that objects can be properly garbage collected | æ¸¬è©¦å°è±¡å¯ä»¥æ­£ç¢ºåƒåœ¾å›æ”¶
            del aggregator1, aggregator2
            gc.collect()
            
            return {'handled': True, 'recovered': True, 'critical_failure': False}
        except Exception:
            return {'handled': False, 'recovered': False, 'critical_failure': True}
    
    def _test_resource_exhaustion_simulation(self) -> Dict[str, Any]:
        """Test resource exhaustion simulation | æ¸¬è©¦è³‡æºè€—ç›¡æ¨¡æ“¬"""
        try:
            # Simulate resource exhaustion by creating many objects | é€šéå‰µå»ºè¨±å¤šå°è±¡ä¾†æ¨¡æ“¬è³‡æºè€—ç›¡
            large_objects = []
            for _ in range(100):
                obj = SignalAggregator()
                signals = self.create_high_volume_signals(100)
                for signal in signals:
                    obj.add_signal(signal)
                large_objects.append(obj)
            
            # Clean up | æ¸…ç†
            del large_objects
            gc.collect()
            
            return {'handled': True, 'recovered': True, 'critical_failure': False}
        except MemoryError:
            # Memory error is expected behavior | è¨˜æ†¶é«”éŒ¯èª¤æ˜¯é æœŸè¡Œç‚º
            gc.collect()
            return {'handled': True, 'recovered': True, 'critical_failure': False}
        except Exception:
            return {'handled': False, 'recovered': False, 'critical_failure': True}
    
    def _test_timing_edge_cases(self) -> Dict[str, Any]:
        """Test timing-related edge cases | æ¸¬è©¦æ™‚é–“ç›¸é—œçš„é‚Šç·£æƒ…æ³"""
        try:
            # Test with signals from very different times | ç”¨ä¾†è‡ªä¸åŒæ™‚é–“çš„ä¿¡è™Ÿæ¸¬è©¦
            old_time = datetime.now() - timedelta(days=365)
            future_time = datetime.now() + timedelta(days=365)
            
            timing_signals = [
                TradingSignal(SignalType.BUY, 0.7, 0.8, "old_source", old_time),
                TradingSignal(SignalType.SELL, 0.6, 0.7, "future_source", future_time)
            ]
            
            # Test confidence calculation with extreme timing | ç”¨æ¥µç«¯æ™‚é–“æ¸¬è©¦ä¿¡å¿ƒè¨ˆç®—
            confidence = self.confidence_scorer.calculate_comprehensive_confidence(timing_signals)
            
            return {'handled': True, 'recovered': True, 'critical_failure': False}
        except Exception:
            return {'handled': False, 'recovered': False, 'critical_failure': True}
    
    def _test_concurrent_modifications(self) -> Dict[str, Any]:
        """Test concurrent modifications handling | æ¸¬è©¦ä¸¦ç™¼ä¿®æ”¹è™•ç†"""
        try:
            # Simulate concurrent modifications | æ¨¡æ“¬ä¸¦ç™¼ä¿®æ”¹
            aggregator = SignalAggregator()
            
            def modifier_thread():
                signals = self.create_high_volume_signals(10)
                for signal in signals:
                    aggregator.add_signal(signal)
                    time.sleep(0.001)
            
            # Start modifier thread | é–‹å§‹ä¿®æ”¹ç·šç¨‹
            thread = threading.Thread(target=modifier_thread)
            thread.start()
            
            # Try to access aggregator while it's being modified | å˜—è©¦åœ¨ä¿®æ”¹æ™‚è¨ªå•èšåˆå™¨
            for _ in range(10):
                try:
                    summary = aggregator.get_summary()
                    time.sleep(0.001)
                except Exception:
                    pass  # Expected during concurrent access | ä¸¦ç™¼è¨ªå•æœŸé–“é æœŸ
            
            thread.join()
            return {'handled': True, 'recovered': True, 'critical_failure': False}
        except Exception:
            return {'handled': False, 'recovered': False, 'critical_failure': True}
    
    def run_all_durability_tests(self) -> Dict[str, Any]:
        """Run all durability tests and generate comprehensive report | é‹è¡Œæ‰€æœ‰è€ä¹…æ€§æ¸¬è©¦ä¸¦ç”Ÿæˆç¶œåˆå ±å‘Š"""
        logger.info("ğŸš€ Starting Signal Combination Durability Test Suite...")
        
        start_time = time.time()
        
        # Test suite | æ¸¬è©¦å¥—ä»¶
        durability_tests = [
            ('Memory Leak Detection', self.test_1_memory_leak_detection),
            ('High Volume Processing', self.test_2_high_volume_processing),
            ('Concurrent Operation Safety', self.test_3_concurrent_operation_safety),
            ('Long-running Stability', self.test_4_long_running_stability),
            ('Extreme Edge Cases', self.test_5_extreme_edge_cases)
        ]
        
        # Execute tests | åŸ·è¡Œæ¸¬è©¦
        test_results = {}
        for test_name, test_function in durability_tests:
            logger.info(f"\n--- Executing: {test_name} ---")
            try:
                test_results[test_name] = test_function()
            except Exception as e:
                logger.error(f"Test {test_name} failed catastrophically: {e}")
                test_results[test_name] = {
                    'status': 'CATASTROPHIC_FAILURE',
                    'error': str(e)
                }
        
        # Generate comprehensive report | ç”Ÿæˆç¶œåˆå ±å‘Š
        total_duration = time.time() - start_time
        return self.generate_durability_report(test_results, total_duration)
    
    def generate_durability_report(self, test_results: Dict[str, Any], 
                                 total_duration: float) -> Dict[str, Any]:
        """Generate comprehensive durability report | ç”Ÿæˆç¶œåˆè€ä¹…æ€§å ±å‘Š"""
        
        # Analyze test results | åˆ†ææ¸¬è©¦çµæœ
        passed_tests = [name for name, result in test_results.items() 
                       if result.get('status') == 'PASSED']
        warning_tests = [name for name, result in test_results.items() 
                        if result.get('status') == 'WARNING']
        failed_tests = [name for name, result in test_results.items() 
                       if result.get('status') in ['FAILED', 'CATASTROPHIC_FAILURE']]
        
        total_tests = len(test_results)
        pass_rate = len(passed_tests) / total_tests * 100 if total_tests > 0 else 0
        
        # Determine overall durability status | ç¢ºå®šæ•´é«”è€ä¹…æ€§ç‹€æ…‹
        if pass_rate >= 80 and len(failed_tests) == 0:
            overall_status = "EXCELLENT_DURABILITY"
            durability_grade = "A"
        elif pass_rate >= 60 and len(failed_tests) <= 1:
            overall_status = "GOOD_DURABILITY"
            durability_grade = "B"
        elif pass_rate >= 40:
            overall_status = "MODERATE_DURABILITY"
            durability_grade = "C"
        else:
            overall_status = "POOR_DURABILITY"
            durability_grade = "D"
        
        # Compile comprehensive report | ç·¨è­¯ç¶œåˆå ±å‘Š
        durability_report = {
            'test_execution_summary': {
                'total_tests': total_tests,
                'passed_tests': len(passed_tests),
                'warning_tests': len(warning_tests),
                'failed_tests': len(failed_tests),
                'pass_rate': f"{pass_rate:.1f}%",
                'total_duration': f"{total_duration:.2f}s"
            },
            'overall_assessment': {
                'durability_status': overall_status,
                'durability_grade': durability_grade,
                'production_ready': pass_rate >= 70 and len(failed_tests) == 0,
                'high_load_capable': pass_rate >= 80,
                'long_term_stable': pass_rate >= 75
            },
            'detailed_results': test_results,
            'recommendations': self.generate_durability_recommendations(
                test_results, overall_status
            ),
            'system_capabilities_validated': {
                'memory_management': any('Memory' in name and 
                                       test_results[name].get('status') == 'PASSED' 
                                       for name in test_results),
                'high_volume_processing': any('High Volume' in name and 
                                            test_results[name].get('status') == 'PASSED' 
                                            for name in test_results),
                'concurrent_safety': any('Concurrent' in name and 
                                       test_results[name].get('status') == 'PASSED' 
                                       for name in test_results),
                'long_term_stability': any('Long-running' in name and 
                                         test_results[name].get('status') == 'PASSED' 
                                         for name in test_results),
                'error_resilience': any('Edge Cases' in name and 
                                      test_results[name].get('status') in ['PASSED', 'WARNING'] 
                                      for name in test_results)
            }
        }
        
        return durability_report
    
    def generate_durability_recommendations(self, test_results: Dict[str, Any], 
                                          overall_status: str) -> List[str]:
        """Generate recommendations based on durability test results | åŸºæ–¼è€ä¹…æ€§æ¸¬è©¦çµæœç”Ÿæˆå»ºè­°"""
        recommendations = []
        
        # Overall status recommendations | æ•´é«”ç‹€æ…‹å»ºè­°
        if overall_status == "EXCELLENT_DURABILITY":
            recommendations.extend([
                "âœ… System demonstrates excellent durability - ready for production deployment",
                "âœ… ç³»çµ±å±•ç¾å‡ºè‰²çš„è€ä¹…æ€§ - æº–å‚™ç”Ÿç”¢éƒ¨ç½²",
                "ğŸ¯ Consider implementing advanced monitoring for production environment",
                "ğŸ¯ è€ƒæ…®ç‚ºç”Ÿç”¢ç’°å¢ƒå¯¦æ–½é«˜ç´šç›£æ§"
            ])
        elif overall_status == "GOOD_DURABILITY":
            recommendations.extend([
                "âœ… System shows good durability with minor areas for improvement",
                "âœ… ç³»çµ±å±•ç¾è‰¯å¥½çš„è€ä¹…æ€§ï¼Œæœ‰å°‘æ•¸æ”¹é€²ç©ºé–“",
                "ğŸ”§ Address warning conditions before production deployment",
                "ğŸ”§ ç”Ÿç”¢éƒ¨ç½²å‰è™•ç†è­¦å‘Šæ¢ä»¶"
            ])
        else:
            recommendations.extend([
                "âš ï¸ System requires durability improvements before production use",
                "âš ï¸ ç³»çµ±éœ€è¦è€ä¹…æ€§æ”¹é€²æ‰èƒ½ç”¨æ–¼ç”Ÿç”¢",
                "ğŸ”§ Focus on failed test areas for system hardening",
                "ğŸ”§ é‡é»é—œæ³¨å¤±æ•—æ¸¬è©¦é ˜åŸŸä»¥åŠ å›ºç³»çµ±"
            ])
        
        # Specific test recommendations | å…·é«”æ¸¬è©¦å»ºè­°
        for test_name, result in test_results.items():
            if result.get('status') == 'FAILED':
                if 'Memory' in test_name:
                    recommendations.append("ğŸ”§ Investigate memory management and potential leaks")
                    recommendations.append("ğŸ”§ èª¿æŸ¥è¨˜æ†¶é«”ç®¡ç†å’Œæ½›åœ¨æ´©æ¼")
                elif 'High Volume' in test_name:
                    recommendations.append("ğŸ”§ Optimize high-volume processing algorithms")
                    recommendations.append("ğŸ”§ å„ªåŒ–é«˜å®¹é‡è™•ç†æ¼”ç®—æ³•")
                elif 'Concurrent' in test_name:
                    recommendations.append("ğŸ”§ Implement better thread safety mechanisms")
                    recommendations.append("ğŸ”§ å¯¦æ–½æ›´å¥½çš„ç·šç¨‹å®‰å…¨æ©Ÿåˆ¶")
                elif 'Long-running' in test_name:
                    recommendations.append("ğŸ”§ Address long-term stability issues")
                    recommendations.append("ğŸ”§ è§£æ±ºé•·æœŸç©©å®šæ€§å•é¡Œ")
                elif 'Edge Cases' in test_name:
                    recommendations.append("ğŸ”§ Strengthen error handling and recovery mechanisms")
                    recommendations.append("ğŸ”§ åŠ å¼·éŒ¯èª¤è™•ç†å’Œæ¢å¾©æ©Ÿåˆ¶")
        
        return recommendations


def main():
    """Main durability test execution function | ä¸»è¦è€ä¹…æ€§æ¸¬è©¦åŸ·è¡Œå‡½æ•¸"""
    try:
        print("=" * 100)
        print("ğŸ”¬ AIFX PHASE 3.1.1 SIGNAL COMBINATION DURABILITY TEST SUITE")
        print("ğŸ¯ Comprehensive durability and stress testing")
        print("=" * 100)
        
        # Initialize and run durability tests | åˆå§‹åŒ–ä¸¦é‹è¡Œè€ä¹…æ€§æ¸¬è©¦
        durability_tester = SignalCombinationDurabilityTest()
        
        # Execute comprehensive durability test suite | åŸ·è¡Œç¶œåˆè€ä¹…æ€§æ¸¬è©¦å¥—ä»¶
        durability_report = durability_tester.run_all_durability_tests()
        
        # Display comprehensive results | é¡¯ç¤ºç¶œåˆçµæœ
        print(f"\nğŸ“Š DURABILITY TEST EXECUTION SUMMARY:")
        print(f"Total Tests: {durability_report['test_execution_summary']['total_tests']}")
        print(f"Passed: {durability_report['test_execution_summary']['passed_tests']}")
        print(f"Warnings: {durability_report['test_execution_summary']['warning_tests']}")
        print(f"Failed: {durability_report['test_execution_summary']['failed_tests']}")
        print(f"Pass Rate: {durability_report['test_execution_summary']['pass_rate']}")
        print(f"Total Duration: {durability_report['test_execution_summary']['total_duration']}")
        
        print(f"\nğŸ¯ OVERALL DURABILITY ASSESSMENT:")
        print(f"Status: {durability_report['overall_assessment']['durability_status']}")
        print(f"Grade: {durability_report['overall_assessment']['durability_grade']}")
        print(f"Production Ready: {durability_report['overall_assessment']['production_ready']}")
        print(f"High Load Capable: {durability_report['overall_assessment']['high_load_capable']}")
        print(f"Long-term Stable: {durability_report['overall_assessment']['long_term_stable']}")
        
        print(f"\nğŸ” SYSTEM CAPABILITIES VALIDATED:")
        capabilities = durability_report['system_capabilities_validated']
        for capability, status in capabilities.items():
            status_icon = "âœ…" if status else "âŒ"
            print(f"{status_icon} {capability.replace('_', ' ').title()}: {'VALIDATED' if status else 'FAILED'}")
        
        print(f"\nğŸ’¡ DURABILITY RECOMMENDATIONS:")
        for rec in durability_report['recommendations']:
            print(f"{rec}")
        
        print("=" * 100)
        
        # Return appropriate exit code | è¿”å›é©ç•¶çš„é€€å‡ºä»£ç¢¼
        if durability_report['overall_assessment']['production_ready']:
            print("ğŸ‰ DURABILITY TEST SUITE: PASSED - System ready for production!")
            return 0
        else:
            print("âš ï¸ DURABILITY TEST SUITE: NEEDS ATTENTION - Address issues before production")
            return 1
        
    except Exception as e:
        logger.error(f"ğŸ’¥ Durability test execution failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)